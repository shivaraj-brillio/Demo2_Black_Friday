{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e2be7f-691a-4429-9784-7dc4ac7d7920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp.dsl import (component, pipeline, Input, Output, Dataset, Model, Artifact, Metrics)\n",
    "\n",
    "@component\n",
    "def ingest(raw_data: Output[Dataset]):\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'google-cloud-storage', 'pandas'], check=True)\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    from io import BytesIO\n",
    "    import pandas as pd\n",
    "\n",
    "    # Initialize a Cloud Storage client\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Define the bucket and the blob (file)\n",
    "    bucket = storage_client.bucket(\"final_demo2_blackfriday\")\n",
    "    blob = bucket.blob(\"train.csv\")\n",
    "    \n",
    "    # Download the file as bytes\n",
    "    black_friday_data = blob.download_as_bytes()\n",
    "    \n",
    "    # Read the bytes into a pandas DataFrame\n",
    "    black_friday_df = pd.read_csv(BytesIO(black_friday_data))\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    black_friday_df.to_csv(raw_data.path, index=False)\n",
    "    \n",
    "@component\n",
    "def feature_selection(\n",
    "    engineered_data: Input[Dataset],\n",
    "    finalized_features_data: Output[Dataset]\n",
    "):\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'pandas', 'scikit-learn', 'google-cloud-storage'], check=True)\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    import json\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def feature_selection_with_k_selection(df, target_column):\n",
    "        \"\"\"\n",
    "        Perform feature selection on a DataFrame with categorical features and a numerical target, and select the best k value.\n",
    "        \"\"\"\n",
    "        X = df.drop(columns=[target_column])\n",
    "        y = df[target_column]\n",
    "\n",
    "        # Define the range of k values to test\n",
    "        k_values = list(range(1, X.shape[1] + 1))\n",
    "\n",
    "        # Create a pipeline with SelectKBest and a RandomForestRegressor\n",
    "        pipeline = Pipeline([\n",
    "            ('selector', SelectKBest(score_func=f_regression)),\n",
    "            ('model', RandomForestRegressor(random_state=42))\n",
    "        ])\n",
    "\n",
    "        # Define the parameter grid\n",
    "        param_grid = {'selector__k': k_values}\n",
    "\n",
    "        # Use GridSearchCV to find the best k value\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        # Get the best k value\n",
    "        best_k = grid_search.best_params_['selector__k']\n",
    "        print(f\"Best k value: {best_k}\")\n",
    "\n",
    "        # Fit the selector with the best k value\n",
    "        selector = SelectKBest(score_func=f_regression, k=best_k)\n",
    "        X_new = selector.fit_transform(X, y)\n",
    "\n",
    "        # Get the selected feature names\n",
    "        selected_features = X.columns[selector.get_support(indices=True)].tolist()\n",
    "\n",
    "        # Create a DataFrame with the selected features\n",
    "        selected_df = pd.DataFrame(X_new, columns=selected_features, index=df.index)\n",
    "\n",
    "        return selected_df, selected_features, best_k\n",
    "\n",
    "    # Load the data\n",
    "    black_friday_df = pd.read_csv(engineered_data.path)\n",
    "\n",
    "    # Perform feature selection\n",
    "    selected_df, selected_features, best_k = feature_selection_with_k_selection(black_friday_df, 'Purchase')\n",
    "\n",
    "    # Add the target column back to the selected features DataFrame\n",
    "    selected_df['Purchase'] = black_friday_df['Purchase']\n",
    "\n",
    "    # Save the selected features to a JSON file\n",
    "    selected_feature_names = selected_features\n",
    "    local_model_path = \"selected_features_names.json\"\n",
    "    with open(local_model_path, 'w') as f:\n",
    "        json.dump(selected_feature_names, f)\n",
    "\n",
    "    # Upload the selected features JSON to GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket('final_demo2_blackfriday')\n",
    "    blob = bucket.blob('selected_features_names.json')\n",
    "    blob.upload_from_filename(local_model_path)\n",
    "\n",
    "    # Save the selected features DataFrame to a CSV file\n",
    "    selected_df.to_csv(finalized_features_data.path, index=False)\n",
    "\n",
    "@component\n",
    "def train_validation_test_split(\n",
    "    finalized_features_data: Input[Dataset],\n",
    "    train_data: Output[Dataset],\n",
    "    validation_data: Output[Dataset],\n",
    "    test_data: Output[Dataset]\n",
    "):\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'pandas', 'scikit-learn'], check=True)\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split as sk_train_test_split\n",
    "\n",
    "    # Load the data\n",
    "    black_friday_df = pd.read_csv(finalized_features_data.path)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    train_df, test_df = sk_train_test_split(black_friday_df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Further split the train data into train and validation sets\n",
    "    train_df, validation_df = sk_train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Save the train, validation, and test sets to CSV files\n",
    "    train_df.to_csv(train_data.path, index=False)\n",
    "    validation_df.to_csv(validation_data.path, index=False)\n",
    "    test_df.to_csv(test_data.path, index=False)\n",
    "\n",
    "@component\n",
    "def hyperparameter_tuning(\n",
    "    validation_data: Input[Dataset],\n",
    "    best_params: Output[Artifact]\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning on the validation dataset using Optuna.\n",
    "    \n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'pandas', 'scikit-learn', 'xgboost==1.6', 'optuna'], check=True)\n",
    "\n",
    "    import optuna\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from functools import partial\n",
    "    import json\n",
    "\n",
    "    # Load the validation dataset\n",
    "    validation_df = pd.read_csv(validation_data.path)\n",
    "    X_val = validation_df.drop('Purchase', axis=1)\n",
    "    y_val = validation_df['Purchase']\n",
    "\n",
    "    def objective(trial, X, y):\n",
    "        \"\"\"\n",
    "        Objective function for Optuna hyperparameter optimization.\n",
    "\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "            \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 1.0),\n",
    "            \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 1.0),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n",
    "            \"eta\": trial.suggest_loguniform(\"eta\", 1e-8, 1.0),\n",
    "            \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 1.0),\n",
    "            \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "            \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "            \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 1e-8, 1.0),\n",
    "            \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 0, 10),\n",
    "            \"scale_pos_weight\": trial.suggest_loguniform(\"scale_pos_weight\", 1e-8, 1.0),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "            \"sampling_method\": trial.suggest_categorical(\"sampling_method\", [\"uniform\"])\n",
    "        }\n",
    "\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "        rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "        return rmse\n",
    "\n",
    "    # Create and optimize the study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(partial(objective, X=X_val, y=y_val), n_trials=50)\n",
    "    best_params_dict = study.best_params\n",
    "\n",
    "    # Save the best parameters to a JSON file\n",
    "    best_params_path = best_params.path + \".json\"\n",
    "    with open(best_params_path, 'w') as f:\n",
    "        json.dump(best_params_dict, f)\n",
    "        \n",
    "@component\n",
    "def model_building(\n",
    "    train_data: Input[Dataset],\n",
    "    best_params: Input[Artifact],\n",
    "    model_output: Output[Model]\n",
    "):\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'pandas','scikit-learn', 'xgboost==1.6', 'joblib', 'google-cloud-storage'], check=True)\n",
    "    \n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    import joblib\n",
    "    import json\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Load the train data\n",
    "    train_df = pd.read_csv(train_data.path)\n",
    "    X_train = train_df.drop('Purchase', axis=1)\n",
    "    y_train = train_df['Purchase']\n",
    "\n",
    "    # Load best parameters\n",
    "    best_params_path = best_params.path + \".json\"\n",
    "    with open(best_params_path, 'r') as f:\n",
    "        best_params_dict = json.load(f)\n",
    "\n",
    "    # Train the model using best parameters\n",
    "    model = xgb.XGBRegressor(**best_params_dict)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model\n",
    "    local_model_path = \"/tmp/model.joblib\"\n",
    "    joblib.dump(model, local_model_path)\n",
    "    \n",
    "    # Upload the model to GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket('final_demo2_blackfriday')\n",
    "    blob = bucket.blob('model.joblib')\n",
    "    blob.upload_from_filename(local_model_path) \n",
    "    \n",
    "     # Output the model artifact\n",
    "    model_output.uri = f\"gs://{bucket.name}/model.joblib\"\n",
    "        \n",
    "@component\n",
    "def upload_model_to_vertex_ai(\n",
    "    model_output: Input[Model],\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    display_name: str,\n",
    "    serving_image: str,\n",
    "    parent_model: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload a trained model to Vertex AI Model Registry.\n",
    "\n",
    "    Args:\n",
    "        model_output (Input[Model]): The input model artifact.\n",
    "        project_id (str): GCP project ID.\n",
    "        region (str): The region for Vertex AI.\n",
    "        display_name (str): Display name for the model in Vertex AI.\n",
    "        serving_image (str): The container image URI for serving the model.\n",
    "        parent_model (str): Parent model ID, if applicable.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'google-cloud-aiplatform', 'google-cloud-storage'], check=True)\n",
    "    \n",
    "    import os\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    # Define the GCS bucket and file details\n",
    "    model_gcs_uri = model_output.uri\n",
    "\n",
    "    # Upload model to Vertex AI Model Registry\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=os.path.dirname(model_gcs_uri),\n",
    "        serving_container_image_uri=serving_image,\n",
    "        serving_container_ports=[5005],\n",
    "        serving_container_health_route=\"/health\",\n",
    "        serving_container_predict_route=\"/predict\",\n",
    "        parent_model=parent_model if parent_model.lower() != 'none' else None,\n",
    "        sync=True\n",
    "    )\n",
    "\n",
    "@component\n",
    "def model_evaluation(\n",
    "    train_data: Input[Dataset],\n",
    "    test_data: Input[Dataset],\n",
    "    model_output: Input[Model],\n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on training and test datasets, and log metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'pandas', 'xgboost==1.6', 'joblib', 'scikit-learn', 'google-cloud-storage'], check=True)\n",
    "    \n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    import joblib\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "\n",
    "    # Load the train and test data\n",
    "    train_df = pd.read_csv(train_data.path)\n",
    "    X_train = train_df.drop('Purchase', axis=1)\n",
    "    y_train = train_df['Purchase']\n",
    "    \n",
    "    test_df = pd.read_csv(test_data.path)\n",
    "    X_test = test_df.drop('Purchase', axis=1)\n",
    "    y_test = test_df['Purchase']\n",
    "\n",
    "    # Load the model from GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket('final_demo2_blackfriday')\n",
    "    blob = bucket.blob('model.joblib')\n",
    "    local_model_path = \"/tmp/model.joblib\"\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(local_model_path), exist_ok=True)\n",
    "    \n",
    "    # Download the model\n",
    "    blob.download_to_filename(local_model_path)\n",
    "    \n",
    "    # Load the model\n",
    "    model = joblib.load(local_model_path)\n",
    "    \n",
    "    # Predict and evaluate on train data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    # Predict and evaluate on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    metrics.log_metric(\"Train RMSE\", train_rmse)\n",
    "    metrics.log_metric(\"Train R2\", train_r2)\n",
    "    metrics.log_metric(\"Test RMSE\", test_rmse)\n",
    "    metrics.log_metric(\"Test R2\", test_r2)\n",
    "\n",
    "# Load components from the separate file\n",
    "from preprocessing_module import preprocessing, feature_engineering\n",
    "\n",
    "@pipeline(\n",
    "    name='black_friday_purchase_model_training_pipeline',\n",
    "    description='A pipeline that processes data from Black Friday sales and builds a predictive model.',\n",
    ")\n",
    "def black_friday_purchase_model_training_pipeline(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    display_name: str,\n",
    "    serving_image: str,\n",
    "    parent_model: str\n",
    "):\n",
    "    ingest_task = ingest()\n",
    "    preprocessed_data = preprocessing(raw_data=ingest_task.output)\n",
    "    engineered_data = feature_engineering(preprocessed_data=preprocessed_data.output)\n",
    "    selected_features_data = feature_selection(engineered_data=engineered_data.output)\n",
    "    \n",
    "    split_data = train_validation_test_split(\n",
    "        finalized_features_data=selected_features_data.output\n",
    "    )\n",
    "    \n",
    "    tuned_params = hyperparameter_tuning(\n",
    "        validation_data=split_data.outputs['validation_data']\n",
    "    )\n",
    "    \n",
    "    trained_model = model_building(\n",
    "        train_data=split_data.outputs['train_data'],\n",
    "        best_params=tuned_params.output\n",
    "    )\n",
    "    \n",
    "    # Upload model to Vertex AI Model Registry\n",
    "    upload_model_task = upload_model_to_vertex_ai(\n",
    "        model_output=trained_model.outputs['model_output'],\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        display_name=display_name,\n",
    "        serving_image=serving_image,\n",
    "        parent_model=parent_model\n",
    "    )\n",
    "    \n",
    "    model_evaluation(\n",
    "        train_data=split_data.outputs['train_data'],\n",
    "        test_data=split_data.outputs['test_data'],\n",
    "        model_output=trained_model.output\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Compile the updated pipeline\n",
    "    kfp.compiler.Compiler().compile(\n",
    "        black_friday_purchase_model_training_pipeline,\n",
    "        'black_friday_purchase_model_training_pipeline.yaml'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90964dc-90cd-482f-abf3-9340f7d5538c",
   "metadata": {},
   "source": [
    "# Deploying to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1233d82a-7e7e-4b4f-8d1a-e4c6d38fef20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/971203737354/locations/us-central1/endpoints/5150514885758550016/operations/2505232055567122432\n",
      "Endpoint created. Resource name: projects/971203737354/locations/us-central1/endpoints/5150514885758550016\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/971203737354/locations/us-central1/endpoints/5150514885758550016')\n",
      "Deploying model to Endpoint : projects/971203737354/locations/us-central1/endpoints/5150514885758550016\n",
      "Deploy Endpoint model backing LRO: projects/971203737354/locations/us-central1/endpoints/5150514885758550016/operations/1190180964374937600\n",
      "Endpoint model deployed. Resource name: projects/971203737354/locations/us-central1/endpoints/5150514885758550016\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the AI Platform client\n",
    "aiplatform.init(project='brldi-gcpcapabilities-ai-audit', location='us-central1')\n",
    "\n",
    "# Specify the model resource path\n",
    "my_model = aiplatform.Model(\"projects/971203737354/locations/us-central1/models/6291045993831989248\")\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "endpoint = my_model.deploy(\n",
    "    deployed_model_display_name='black_friday_endpoint',\n",
    "    traffic_split={\"0\": 100},\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edd65e-000a-4738-bcca-6ca70e29b5e6",
   "metadata": {},
   "source": [
    "# Online Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63402f83-8445-49d9-8b57-abb16612b337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 2509.952880859375}]\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the AI Platform client\n",
    "aiplatform.init(project='brldi-gcpcapabilities-ai-audit', location='us-central1')\n",
    "endpoint = aiplatform.Endpoint('projects/971203737354/locations/us-central1/endpoints/5150514885758550016')\n",
    "\n",
    "data ={\n",
    "  \"instances\": [\n",
    "    {\n",
    "      \"User_ID\": 1000011,\n",
    "      \"Product_ID\": \"P00053842\",\n",
    "      \"Gender\": \"F\",\n",
    "      \"Age\": \"26-35\",\n",
    "      \"Occupation\": 1,\n",
    "      \"City_Category\": \"C\",\n",
    "      \"Stay_In_Current_City_Years\": 1,\n",
    "      \"Marital_Status\": 0,\n",
    "      \"Product_Category_1\": 4,\n",
    "      \"Product_Category_2\": 5,\n",
    "      \"Product_Category_3\": 12\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Perform online prediction\n",
    "prediction = endpoint.predict(instances=data['instances']).predictions\n",
    "# Print the prediction results\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3b5ea-195c-475c-a230-47b92bbcebb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
